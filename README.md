# üõçÔ∏èüõíDATA-PIPELINE-LAZADA
M·ªôt h·ªá th·ªëng Pipeline x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ s√†n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Lazada: 
Crawl d·ªØ li·ªáu s·∫£n ph·∫©m ‚Üí L∆∞u tr·ªØ v√†o HDFS (Hadoop Distributed File System) ‚Üí X·ª≠ l√Ω b·∫±ng PySpark ‚Üí L∆∞u k·∫øt qu·∫£ v√†o MongoDB ƒë·ªÉ ph√¢n t√≠ch ho·∫∑c hi·ªÉn th·ªã - tr·ª±c quan h√≥a v√† ph√¢n t√≠ch
--> Xem chi ti·∫øt [b√°o_c√°o]()
## üìÇ M√¥ h√¨nh h·ªá th·ªëng 

```
+------------+       +-------------+        +-----------+        +-------------+
|            |       |             |        |           |        |             |
|  Crawler   +-----> |  HDFS (Hadoop)+----> |  PySpark  +------> |   MongoDB   |
| (Selenium) |       |  (Ubuntu VM) |        | (Processing)      | (Storage)   |
+------------+       +-------------+        +-----------+        +-------------+

```

![image](https://github.com/user-attachments/assets/6b386765-0c1a-4bf1-a584-8cee913db1f4)

## üß∞ C√¥ng ngh·ªá s·ª≠ d·ª•ng
* Web Crawler: Selenium, Requests, BeautifulSoup
* Big Data: Hadoop (HDFS - v·ªõi 1 master + 1 slave node)
* X·ª≠ l√Ω d·ªØ li·ªáu: PySpark
* C∆° s·ªü d·ªØ li·ªáu: MongoDB Atlas
* DevOps: Ubuntu, Window
* Ng√¥n ng·ªØ: Python 3.10+
* ƒêa lu·ªìng: Thread

## ‚öôÔ∏è C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng Hadoop
### 1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng ubuntu
* 1.1 T·∫£i xu·ªëng [ubuntu](https://ubuntu.com/download/desktop/thank-you?version=24.04.1&architecture=amd64&lts=true)
* 1.2 Thi·∫øt l·∫≠p Virualbox ( Network -> Attached to: Bridged Adapter )
* 1.3 Th√™m add file ubuntu l√™n m√°y ·∫£o virualbox
* 1.4 C·∫≠p nh·∫≠t: `sudo apt update`
  - C√†i java ph√π h·ª£p v·ªõi hadoop `sudo apt install openjdk-8-jdk -y`
  - Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n m√¥i tr∆∞·ªùng `which javac ` v√† `readlink -f path` path l√† k·∫øt qu·∫£ which javac v√† ƒë∆∞·ªùng d·∫´n java th∆∞·ªùng l√† `/usr/lib/jvm/java-8-openjdk-amd64`
### 2. T·∫£i hadoop 
* 2.1 terminal run `wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz`
* 2.2 Gi·∫£i n√©n t·ªáp: tar xzf hadoop-3.4.0.tar.gz
* 2.4 Chuy·ªÉn file hadoop-3.4.0 ƒë√£ ƒë∆∞·ª£c gi·∫£i n√©n v·ªÅ home trong ubuntu 
![image](https://github.com/user-attachments/assets/caa72312-7a98-4d5c-bd02-c826e70a1220)

- C√†i OpenSSH t·∫°o kh√≥a: `sudo apt install openssh-server openssh-client ‚Äìy`
- T·∫°o v√† c√†i ƒë·∫∑t SSH Certificates: `ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa`
- L∆∞u l·∫°i th√¥ng tin kh√≥a:  `cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`
- C·∫•p quy·ªÅn: `chmod 0600 ~/.ssh/authorized_keys`
- Ki·ªÉm tra ssh: `ssh localhost` nh·ªõ IP
  ![image](https://github.com/user-attachments/assets/251dc379-03bb-47cf-af79-66471fe45637)
  
#### C·∫•u h√¨nh c√°c file Hadoop
M·ªü terminal m·ªü file `cd hadoop-3.4.0` truy c·∫≠p hadoop `cd etc/hadoop`
* **C·∫•u h√¨nh: bashrc** ( th√™m c√°c bi·∫øn m√¥i tr∆∞·ªùng ) 
  - `gedit ~/.bashrc`
  - C·∫•u h√¨nh file v√† th√™m v√†o c√°c bi·∫øn m√¥i tr∆∞·ªùng trong bashrc
```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/home/admin/hadoop-3.4.0
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
```
L∆∞u √Ω: admin l√† t√™n user_admin 
![image](https://github.com/user-attachments/assets/7c422198-4e17-4c6f-9b40-9d7234b87c93)

  - L∆∞u thay ƒë·ªïi bashrc `source ~/.bashrc

* **C·∫•u h√¨nh file hadoop-env.sh** ( m√¥i tr∆∞·ªùng c·ªßa hadoop )
  - `gedit hadoop-env.sh`
  - Th√™m ƒë·ªãa ch·ªâ v√†o d√≤ng 28 d∆∞·ªõi #JAVA_HOME
  - `export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64`
    ![image](https://github.com/user-attachments/assets/eb6a1c25-a1a8-4d78-a10a-0c07df60c006)
  - T·∫°o th∆∞ m·ª•c trung chuy·ªÉn ( th∆∞ m·ª•c t·∫°m )
  - `mkdir -p /home/admin/hadoop-3.3.1/tmp`
    ![image](https://github.com/user-attachments/assets/fcc7df1f-b22b-4992-abdb-d7fe3b17f282)
    
* **C·∫•u h√¨nh file core-site.xml**
  - `gedit core-site.xml`

```
<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/admin/hadoop-3.4.0/tmp</value>
</property>	
<property>
<name>fs.default.name</name	>
<value>hdfs://localhost:9000</value>
</property>
</configuration>
```
**hadoop.tmp.dir**
- D√πng ƒë·ªÉ l∆∞u tr·ªØ file t·∫°m, d·ªØ li·ªáu trung gian, metadata, ho·∫∑c m·ªôt s·ªë d·ªØ li·ªáu c·∫ßn thi·∫øt trong qu√° tr√¨nh th·ª±c thi Hadoop.
- L√† n∆°i l∆∞u c√°c file t·∫°m c·ªßa c√°c d·ªãch v·ª• nh∆∞ NameNode, DataNode, SecondaryNameNode, v.v.
- Th∆∞ m·ª•c n√†y n√™n t·ªìn t·∫°i s·∫µn v√† Hadoop ph·∫£i c√≥ quy·ªÅn ghi v√†o.
 **fs.defaultFS**
  - ƒê·ªãnh nghƒ©a ƒë·ªãa ch·ªâ m·∫∑c ƒë·ªãnh c·ªßa h·ªá th·ªëng file m√† Hadoop s·ª≠ d·ª•ng
  - N√≥ cho Hadoop bi·∫øt n√™n giao ti·∫øp v·ªõi h·ªá th·ªëng file n√†o ƒë·ªÉ ƒë·ªçc/ghi d·ªØ li·ªáu.
  - hdfs://localhost:9000 nghƒ©a l√† Hadoop s·∫Ω k·∫øt n·ªëi t·ªõi NameNode ƒëang ch·∫°y ·ªü localhost (t·ª©c m√°y c·ª•c b·ªô) tr√™n port 9000.

* **C·∫•u h√¨nh Mapred-site.xml**
- `gedit mapred-site.xml`
```
<configuration>
<property>
  <name>mapreduce.framework.name</name> 
  <value>yarn</value> 
</property>
</configuration>
```
- C·∫•u h√¨nh tr√™n ch·ªâ ƒë·ªãnh framework m√† MapReduce s·ª≠ d·ª•ng ƒë·ªÉ th·ª±c thi c√°c job
* **T·∫°o hai folder NameNode v√† DataNode**
`mkdir -p /home/admin/hadoop-3.3.1/data/namenode`
`mkdir -p /home/admin/hadoop-3.3.1/data/datanode`

* **C·∫•u h√¨nh file hdfs-site.xml**
`gedit hdfs-site.xml `

```
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>/home/admin/hadoop-3.3.1/data/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>/home/admin/hadoop-3.4.0/data/datanode</value>
</property>
<property>
<name>dfs.namenode.rpc-address</name>
<value>0.0.0.0:9000 </value>
</property>
<property>
<name>dfs.webhdfs.enabled </name>
<value>true </value>
</property>
```
**Gi·∫£i th√≠ch**
  - Replication : s·ªë b·∫£n sao block c√≥ trong rack
  - Namenode.name: x√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô tr√™n m√°y ch·ªß NameNode n∆°i metadata c·ªßa HDFS (th√¥ng tin v·ªÅ c·∫•u tr√∫c file, v·ªã tr√≠ c·ªßa c√°c block) s·∫Ω ƒë∆∞·ª£c l∆∞u tr·ªØ.
  - Datanode.data: ch·ªâ ra ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô tr√™n c√°c m√°y DataNode n∆°i d·ªØ li·ªáu th·ª±c t·∫ø (c√°c block d·ªØ li·ªáu) s·∫Ω ƒë∆∞·ª£c l∆∞u tr·ªØ.
  - Namenode.rpc-address: X√°c ƒë·ªãnh ƒë·ªãa ch·ªâ v√† c·ªïng giao ti·∫øp m√† NameNode s·∫Ω l·∫Øng nghe c√°c y√™u c·∫ßu t·ª´ client v√† c√°c node kh√°c.
  - Webhdfs.enabled: b·∫≠t ho·∫∑c t·∫Øt t√≠nh nƒÉng WebHDFS, cho ph√©p t∆∞∆°ng t√°c v·ªõi HDFS th√¥ng qua c√°c y√™u c·∫ßu HTTP (REST API).

![image](https://github.com/user-attachments/assets/a3acf5dc-b170-437e-9ead-5108b8ebc825)

* **C·∫•u h√¨nh file Yarn-site.xml**
  - M·ªü file trong terminal `gedit yarn-site.xml`

```
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>

<property>
  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>127.0.0.1</value>
</property>
<property>
  <name>yarn.acl.enable</name>
  <value>0</value>
</property>
<property>
  <name>yarn.nodemanager.env-whitelist</name>   
<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
```
**Gi·∫£i Th√≠ch**
 - `yarn.resourcemanager.hostname` Gi√∫p c√°c NodeManager bi·∫øt ƒë·ªãa ch·ªâ c·ªßa ResourceManager ƒë·ªÉ g·ª≠i th√¥ng tin t√†i nguy√™n, nh·∫≠n job,...
 - `yarn.acl.enable` B·∫≠t/t·∫Øt t√≠nh nƒÉng Access Control List (ACL) ‚Äî t·ª©c l√† quy·ªÅn truy c·∫≠p ng∆∞·ªùi d√πng
 - `yarn.nodemanager.env-whitelist` Danh s√°ch c√°c bi·∫øn m√¥i tr∆∞·ªùng (environment variables) ƒë∆∞·ª£c cho ph√©p s·ª≠ d·ª•ng trong NodeManager

* **Ch·∫°y h·ªá th·ªëng**
  - Ch·∫°y l·ªánh `hdfs namenode ‚Äìformat`
  - Ch·∫°y to√†n b·ªô `start-all.sh`
  - Ki·ªÉm tra jps (_c√≥ ƒë·ªß 6 ch∆∞∆°ng tr√¨nh ch·∫°y_) 
  - C√≥ th·ªÉ truy c·∫≠p HDFS tr√™n UI `localhost:9870` truy c·∫≠p Yarn `localhost:8088`
**ƒê·ªçc th√™m chi ti·∫øt t·∫°i file docx**
